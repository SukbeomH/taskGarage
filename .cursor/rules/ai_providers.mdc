---
alwaysApply: true
---

# Task Garage AI 제공자 관리 가이드

## **AI 제공자 아키텍처**

### **전체 구조**
```
AI Providers
├── base-provider.js           # 기본 제공자 클래스
├── anthropic.js              # Anthropic Claude
├── openai.js                 # OpenAI GPT
├── gemini.js                 # Google Gemini
├── perplexity.js             # Perplexity AI
├── mistral.js                # Mistral AI
├── azure.js                  # Azure OpenAI
├── openrouter.js             # OpenRouter
├── xai.js                    # xAI Grok
├── ollama.js                 # Ollama
└── custom-sdk/               # 커스텀 SDK 구현
```

### **기본 제공자 클래스**
```javascript
// ✅ DO: 기본 제공자 클래스 상속
import { BaseProvider } from './base-provider.js';

export class AnthropicProvider extends BaseProvider {
    constructor(config) {
        super(config);
        this.providerName = 'anthropic';
        this.supportedModels = [
            'claude-3-opus-20240229',
            'claude-3-sonnet-20240229',
            'claude-3-haiku-20240307'
        ];
    }
    
    async generateText(params) {
        // Anthropic 특화 구현
        return await this.callAnthropicAPI(params);
    }
    
    async generateObject(params) {
        // 구조화된 출력 구현
        return await this.callAnthropicObjectAPI(params);
    }
}
```

## **모델 구성 관리**

### **모델 설정 구조**
```javascript
// ✅ DO: 표준 모델 설정 구조
const MODEL_CONFIG = {
    main: {
        provider: 'anthropic',
        model: 'claude-3-sonnet-20240229',
        maxTokens: 4000,
        temperature: 0.7
    },
    research: {
        provider: 'perplexity',
        model: 'llama-3.1-70b-instruct',
        maxTokens: 2000,
        temperature: 0.5
    },
    fallback: {
        provider: 'openai',
        model: 'gpt-4o-mini',
        maxTokens: 2000,
        temperature: 0.7
    }
};
```

### **모델 설정 가져오기**
```javascript
// ✅ DO: 모델 설정 표준 접근
import { getConfiguredModel, getModelParameters } from '../config-manager.js';

async function getOptimalModel(taskType) {
    const modelConfig = getConfiguredModel(taskType);
    const params = getModelParameters(taskType);
    
    return {
        provider: modelConfig.provider,
        model: modelConfig.model,
        maxTokens: params.maxTokens,
        temperature: params.temperature
    };
}
```

## **AI 제공자 구현**

### **새로운 제공자 추가**
```javascript
// ✅ DO: 새로운 AI 제공자 구현
import { BaseProvider } from './base-provider.js';
import { generateText, generateObject } from '@ai-sdk/vercel';

export class NewProvider extends BaseProvider {
    constructor(config) {
        super(config);
        this.providerName = 'new-provider';
        this.supportedModels = [
            'model-1',
            'model-2'
        ];
    }
    
    async generateText(params) {
        const { prompt, model, maxTokens, temperature } = params;
        
        try {
            const result = await generateText({
                model: `${this.providerName}:${model}`,
                prompt,
                max_tokens: maxTokens,
                temperature
            });
            
            return {
                text: result.text,
                usage: result.usage
            };
        } catch (error) {
            throw this.handleProviderError(error);
        }
    }
    
    async generateObject(params) {
        const { prompt, schema, model, maxTokens, temperature } = params;
        
        try {
            const result = await generateObject({
                model: `${this.providerName}:${model}`,
                prompt,
                schema,
                max_tokens: maxTokens,
                temperature
            });
            
            return {
                object: result.object,
                usage: result.usage
            };
        } catch (error) {
            throw this.handleProviderError(error);
        }
    }
    
    handleProviderError(error) {
        // 제공자별 에러 처리
        if (error.code === 'RATE_LIMIT') {
            return new Error('속도 제한에 도달했습니다');
        }
        
        return error;
    }
}
```

### **API 키 관리**
```javascript
// ✅ DO: API 키 표준 관리
import { resolveEnvVariable } from '../utils.js';

export class SecureProvider extends BaseProvider {
    constructor(config) {
        super(config);
        this.apiKey = this.resolveAPIKey();
    }
    
    resolveAPIKey() {
        // 환경 변수에서 API 키 해결
        const apiKey = resolveEnvVariable(
            `${this.providerName.toUpperCase()}_API_KEY`,
            this.session
        );
        
        if (!apiKey) {
            throw new Error(`${this.providerName} API 키가 설정되지 않았습니다`);
        }
        
        return apiKey;
    }
    
    getHeaders() {
        return {
            'Authorization': `Bearer ${this.apiKey}`,
            'Content-Type': 'application/json'
        };
    }
}
```

## **모델별 특성 활용**

### **모델 선택 전략**
```javascript
// ✅ DO: 작업 유형별 모델 선택
function selectModelForTask(taskType, complexity) {
    switch (taskType) {
        case 'creative':
            return getConfiguredModel('main'); // Claude for creativity
            
        case 'research':
            return getConfiguredModel('research'); // Perplexity for research
            
        case 'code':
            if (complexity === 'high') {
                return getConfiguredModel('main'); // Claude for complex code
            } else {
                return getConfiguredModel('fallback'); // GPT for simple code
            }
            
        case 'analysis':
            return getConfiguredModel('main'); // Claude for analysis
            
        default:
            return getConfiguredModel('fallback'); // Default fallback
    }
}
```

### **모델 성능 최적화**
```javascript
// ✅ DO: 모델별 최적화된 파라미터
const MODEL_OPTIMIZATIONS = {
    'claude-3-opus-20240229': {
        maxTokens: 4000,
        temperature: 0.7,
        systemPrompt: "당신은 고급 AI 어시스턴트입니다."
    },
    'gpt-4o-mini': {
        maxTokens: 2000,
        temperature: 0.5,
        systemPrompt: "당신은 효율적인 AI 어시스턴트입니다."
    },
    'llama-3.1-70b-instruct': {
        maxTokens: 2000,
        temperature: 0.3,
        systemPrompt: "당신은 연구 전문 AI 어시스턴트입니다."
    }
};

function getOptimizedParams(model) {
    return MODEL_OPTIMIZATIONS[model] || {
        maxTokens: 2000,
        temperature: 0.7,
        systemPrompt: "당신은 AI 어시스턴트입니다."
    };
}
```

## **에러 처리 및 재시도**

### **제공자별 에러 처리**
```javascript
// ✅ DO: 제공자별 특화 에러 처리
class ProviderErrorHandler {
    static handleError(error, provider) {
        switch (provider) {
            case 'anthropic':
                return this.handleAnthropicError(error);
                
            case 'openai':
                return this.handleOpenAIError(error);
                
            case 'perplexity':
                return this.handlePerplexityError(error);
                
            default:
                return this.handleGenericError(error);
        }
    }
    
    static handleAnthropicError(error) {
        if (error.code === 'rate_limit_exceeded') {
            return new Error('Anthropic 속도 제한에 도달했습니다. 잠시 후 다시 시도하세요.');
        }
        
        if (error.code === 'model_not_found') {
            return new Error('지정된 Claude 모델을 찾을 수 없습니다.');
        }
        
        return error;
    }
    
    static handleOpenAIError(error) {
        if (error.code === 'insufficient_quota') {
            return new Error('OpenAI 할당량이 부족합니다.');
        }
        
        return error;
    }
}
```

### **자동 재시도 로직**
```javascript
// ✅ DO: 지능적 재시도 로직
async function retryWithFallback(primaryCall, fallbackCall, maxRetries = 3) {
    let lastError;
    
    // 주 모델로 시도
    for (let attempt = 1; attempt <= maxRetries; attempt++) {
        try {
            return await primaryCall();
        } catch (error) {
            lastError = error;
            
            // 재시도 가능한 에러인지 확인
            if (!isRetryableError(error)) {
                break;
            }
            
            // 지수 백오프
            await new Promise(resolve => 
                setTimeout(resolve, Math.pow(2, attempt) * 1000)
            );
        }
    }
    
    // 폴백 모델로 시도
    try {
        return await fallbackCall();
    } catch (fallbackError) {
        throw new Error(`모든 모델 시도 실패: ${lastError.message}, ${fallbackError.message}`);
    }
}

function isRetryableError(error) {
    const retryableCodes = [
        'rate_limit_exceeded',
        'timeout',
        'network_error',
        'service_unavailable'
    ];
    
    return retryableCodes.includes(error.code);
}
```

## **성능 모니터링**

### **제공자별 성능 추적**
```javascript
// ✅ DO: 제공자별 성능 모니터링
class ProviderPerformanceMonitor {
    constructor() {
        this.metrics = new Map();
    }
    
    recordCall(provider, model, startTime, endTime, success, tokens) {
        const key = `${provider}:${model}`;
        
        if (!this.metrics.has(key)) {
            this.metrics.set(key, {
                totalCalls: 0,
                successfulCalls: 0,
                failedCalls: 0,
                totalTokens: 0,
                averageResponseTime: 0,
                totalCost: 0
            });
        }
        
        const metric = this.metrics.get(key);
        const responseTime = endTime - startTime;
        
        metric.totalCalls++;
        metric.totalTokens += tokens;
        metric.averageResponseTime = 
            (metric.averageResponseTime * (metric.totalCalls - 1) + responseTime) / metric.totalCalls;
        
        if (success) {
            metric.successfulCalls++;
            metric.totalCost += this.calculateCost(provider, model, tokens);
        } else {
            metric.failedCalls++;
        }
    }
    
    calculateCost(provider, model, tokens) {
        const costRates = {
            'anthropic:claude-3-sonnet-20240229': 0.003 / 1000,
            'openai:gpt-4o-mini': 0.00015 / 1000,
            'perplexity:llama-3.1-70b-instruct': 0.0002 / 1000
        };
        
        const rate = costRates[`${provider}:${model}`] || 0.001 / 1000;
        return tokens * rate;
    }
    
    getMetrics() {
        const result = {};
        
        for (const [key, metric] of this.metrics) {
            result[key] = {
                ...metric,
                successRate: metric.successfulCalls / metric.totalCalls,
                averageTokensPerCall: metric.totalTokens / metric.totalCalls
            };
        }
        
        return result;
    }
}
```

## **설정 관리**

### **모델 설정 업데이트**
```javascript
// ✅ DO: 동적 모델 설정 업데이트
class ModelConfigManager {
    constructor() {
        this.config = this.loadConfig();
    }
    
    loadConfig() {
        try {
            const configPath = path.join(process.cwd(), '.taskmaster', 'config.json');
            return JSON.parse(fs.readFileSync(configPath, 'utf8'));
        } catch (error) {
            return this.getDefaultConfig();
        }
    }
    
    getDefaultConfig() {
        return {
            models: {
                main: {
                    provider: 'anthropic',
                    model: 'claude-3-sonnet-20240229'
                },
                research: {
                    provider: 'perplexity',
                    model: 'llama-3.1-70b-instruct'
                },
                fallback: {
                    provider: 'openai',
                    model: 'gpt-4o-mini'
                }
            },
            parameters: {
                maxTokens: 2000,
                temperature: 0.7
            }
        };
    }
    
    updateModelConfig(role, provider, model) {
        this.config.models[role] = { provider, model };
        this.saveConfig();
    }
    
    saveConfig() {
        const configPath = path.join(process.cwd(), '.taskmaster', 'config.json');
        fs.writeFileSync(configPath, JSON.stringify(this.config, null, 2));
    }
}
```

## **테스트 및 검증**

### **제공자 테스트**
```javascript
// ✅ DO: AI 제공자 단위 테스트
describe('AI Providers', () => {
    let provider;
    
    beforeEach(() => {
        provider = new AnthropicProvider({
            apiKey: 'test-key'
        });
    });
    
    it('should generate text successfully', async () => {
        const result = await provider.generateText({
            prompt: 'Hello, world!',
            model: 'claude-3-sonnet-20240229',
            maxTokens: 100,
            temperature: 0.7
        });
        
        expect(result.text).toBeDefined();
        expect(result.usage).toBeDefined();
    });
    
    it('should handle API errors gracefully', async () => {
        // API 에러 시뮬레이션
        jest.spyOn(provider, 'callAnthropicAPI').mockRejectedValue({
            code: 'rate_limit_exceeded',
            message: 'Rate limit exceeded'
        });
        
        await expect(provider.generateText({
            prompt: 'Test',
            model: 'claude-3-sonnet-20240229'
        })).rejects.toThrow('속도 제한에 도달했습니다');
    });
});
```

### **모델 설정 검증**
```javascript
// ✅ DO: 모델 설정 유효성 검증
function validateModelConfig(config) {
    const errors = [];
    
    // 필수 역할 확인
    const requiredRoles = ['main', 'research', 'fallback'];
    for (const role of requiredRoles) {
        if (!config.models[role]) {
            errors.push(`필수 역할 '${role}'이 설정되지 않았습니다`);
        }
    }
    
    // 모델 유효성 확인
    for (const [role, modelConfig] of Object.entries(config.models)) {
        if (!modelConfig.provider || !modelConfig.model) {
            errors.push(`역할 '${role}'의 provider 또는 model이 설정되지 않았습니다`);
        }
    }
    
    // 파라미터 범위 확인
    if (config.parameters.temperature < 0 || config.parameters.temperature > 2) {
        errors.push('temperature는 0과 2 사이여야 합니다');
    }
    
    if (config.parameters.maxTokens < 1 || config.parameters.maxTokens > 100000) {
        errors.push('maxTokens는 1과 100000 사이여야 합니다');
    }
    
    return {
        isValid: errors.length === 0,
        errors
    };
}
```

---

**참고 파일:**
- [ai-providers/](mdc:src/ai-providers/) - AI 제공자 구현
- [config-manager.js](mdc:scripts/modules/config-manager.js) - 설정 관리
- [ai-services-unified.js](mdc:scripts/modules/ai-services-unified.js) - 통합 AI 서비스

8.  **Documentation:**
    -   Update any relevant documentation (like `README.md` or other rules) mentioning supported providers or configuration.

*(Note: For providers **without** an official Vercel AI SDK adapter, the process would involve directly using the provider's own SDK or API within the `src/ai-providers/<provider-name>.js` module and manually constructing responses compatible with the unified service layer, which is significantly more complex.)*# Task Garage AI 제공자 관리 가이드

## **AI 제공자 아키텍처**

### **전체 구조**
```
AI Providers
├── base-provider.js           # 기본 제공자 클래스
├── anthropic.js              # Anthropic Claude
├── openai.js                 # OpenAI GPT
├── gemini.js                 # Google Gemini
├── perplexity.js             # Perplexity AI
├── mistral.js                # Mistral AI
├── azure.js                  # Azure OpenAI
├── openrouter.js             # OpenRouter
├── xai.js                    # xAI Grok
├── ollama.js                 # Ollama
└── custom-sdk/               # 커스텀 SDK 구현
```

### **기본 제공자 클래스**
```javascript
// ✅ DO: 기본 제공자 클래스 상속
import { BaseProvider } from './base-provider.js';

export class AnthropicProvider extends BaseProvider {
    constructor(config) {
        super(config);
        this.providerName = 'anthropic';
        this.supportedModels = [
            'claude-3-opus-20240229',
            'claude-3-sonnet-20240229',
            'claude-3-haiku-20240307'
        ];
    }
    
    async generateText(params) {
        // Anthropic 특화 구현
        return await this.callAnthropicAPI(params);
    }
    
    async generateObject(params) {
        // 구조화된 출력 구현
        return await this.callAnthropicObjectAPI(params);
    }
}
```

## **모델 구성 관리**

### **모델 설정 구조**
```javascript
// ✅ DO: 표준 모델 설정 구조
const MODEL_CONFIG = {
    main: {
        provider: 'anthropic',
        model: 'claude-3-sonnet-20240229',
        maxTokens: 4000,
        temperature: 0.7
    },
    research: {
        provider: 'perplexity',
        model: 'llama-3.1-70b-instruct',
        maxTokens: 2000,
        temperature: 0.5
    },
    fallback: {
        provider: 'openai',
        model: 'gpt-4o-mini',
        maxTokens: 2000,
        temperature: 0.7
    }
};
```

### **모델 설정 가져오기**
```javascript
// ✅ DO: 모델 설정 표준 접근
import { getConfiguredModel, getModelParameters } from '../config-manager.js';

async function getOptimalModel(taskType) {
    const modelConfig = getConfiguredModel(taskType);
    const params = getModelParameters(taskType);
    
    return {
        provider: modelConfig.provider,
        model: modelConfig.model,
        maxTokens: params.maxTokens,
        temperature: params.temperature
    };
}
```

## **AI 제공자 구현**

### **새로운 제공자 추가**
```javascript
// ✅ DO: 새로운 AI 제공자 구현
import { BaseProvider } from './base-provider.js';
import { generateText, generateObject } from '@ai-sdk/vercel';

export class NewProvider extends BaseProvider {
    constructor(config) {
        super(config);
        this.providerName = 'new-provider';
        this.supportedModels = [
            'model-1',
            'model-2'
        ];
    }
    
    async generateText(params) {
        const { prompt, model, maxTokens, temperature } = params;
        
        try {
            const result = await generateText({
                model: `${this.providerName}:${model}`,
                prompt,
                max_tokens: maxTokens,
                temperature
            });
            
            return {
                text: result.text,
                usage: result.usage
            };
        } catch (error) {
            throw this.handleProviderError(error);
        }
    }
    
    async generateObject(params) {
        const { prompt, schema, model, maxTokens, temperature } = params;
        
        try {
            const result = await generateObject({
                model: `${this.providerName}:${model}`,
                prompt,
                schema,
                max_tokens: maxTokens,
                temperature
            });
            
            return {
                object: result.object,
                usage: result.usage
            };
        } catch (error) {
            throw this.handleProviderError(error);
        }
    }
    
    handleProviderError(error) {
        // 제공자별 에러 처리
        if (error.code === 'RATE_LIMIT') {
            return new Error('속도 제한에 도달했습니다');
        }
        
        return error;
    }
}
```

### **API 키 관리**
```javascript
// ✅ DO: API 키 표준 관리
import { resolveEnvVariable } from '../utils.js';

export class SecureProvider extends BaseProvider {
    constructor(config) {
        super(config);
        this.apiKey = this.resolveAPIKey();
    }
    
    resolveAPIKey() {
        // 환경 변수에서 API 키 해결
        const apiKey = resolveEnvVariable(
            `${this.providerName.toUpperCase()}_API_KEY`,
            this.session
        );
        
        if (!apiKey) {
            throw new Error(`${this.providerName} API 키가 설정되지 않았습니다`);
        }
        
        return apiKey;
    }
    
    getHeaders() {
        return {
            'Authorization': `Bearer ${this.apiKey}`,
            'Content-Type': 'application/json'
        };
    }
}
```

## **모델별 특성 활용**

### **모델 선택 전략**
```javascript
// ✅ DO: 작업 유형별 모델 선택
function selectModelForTask(taskType, complexity) {
    switch (taskType) {
        case 'creative':
            return getConfiguredModel('main'); // Claude for creativity
            
        case 'research':
            return getConfiguredModel('research'); // Perplexity for research
            
        case 'code':
            if (complexity === 'high') {
                return getConfiguredModel('main'); // Claude for complex code
            } else {
                return getConfiguredModel('fallback'); // GPT for simple code
            }
            
        case 'analysis':
            return getConfiguredModel('main'); // Claude for analysis
            
        default:
            return getConfiguredModel('fallback'); // Default fallback
    }
}
```

### **모델 성능 최적화**
```javascript
// ✅ DO: 모델별 최적화된 파라미터
const MODEL_OPTIMIZATIONS = {
    'claude-3-opus-20240229': {
        maxTokens: 4000,
        temperature: 0.7,
        systemPrompt: "당신은 고급 AI 어시스턴트입니다."
    },
    'gpt-4o-mini': {
        maxTokens: 2000,
        temperature: 0.5,
        systemPrompt: "당신은 효율적인 AI 어시스턴트입니다."
    },
    'llama-3.1-70b-instruct': {
        maxTokens: 2000,
        temperature: 0.3,
        systemPrompt: "당신은 연구 전문 AI 어시스턴트입니다."
    }
};

function getOptimizedParams(model) {
    return MODEL_OPTIMIZATIONS[model] || {
        maxTokens: 2000,
        temperature: 0.7,
        systemPrompt: "당신은 AI 어시스턴트입니다."
    };
}
```

## **에러 처리 및 재시도**

### **제공자별 에러 처리**
```javascript
// ✅ DO: 제공자별 특화 에러 처리
class ProviderErrorHandler {
    static handleError(error, provider) {
        switch (provider) {
            case 'anthropic':
                return this.handleAnthropicError(error);
                
            case 'openai':
                return this.handleOpenAIError(error);
                
            case 'perplexity':
                return this.handlePerplexityError(error);
                
            default:
                return this.handleGenericError(error);
        }
    }
    
    static handleAnthropicError(error) {
        if (error.code === 'rate_limit_exceeded') {
            return new Error('Anthropic 속도 제한에 도달했습니다. 잠시 후 다시 시도하세요.');
        }
        
        if (error.code === 'model_not_found') {
            return new Error('지정된 Claude 모델을 찾을 수 없습니다.');
        }
        
        return error;
    }
    
    static handleOpenAIError(error) {
        if (error.code === 'insufficient_quota') {
            return new Error('OpenAI 할당량이 부족합니다.');
        }
        
        return error;
    }
}
```

### **자동 재시도 로직**
```javascript
// ✅ DO: 지능적 재시도 로직
async function retryWithFallback(primaryCall, fallbackCall, maxRetries = 3) {
    let lastError;
    
    // 주 모델로 시도
    for (let attempt = 1; attempt <= maxRetries; attempt++) {
        try {
            return await primaryCall();
        } catch (error) {
            lastError = error;
            
            // 재시도 가능한 에러인지 확인
            if (!isRetryableError(error)) {
                break;
            }
            
            // 지수 백오프
            await new Promise(resolve => 
                setTimeout(resolve, Math.pow(2, attempt) * 1000)
            );
        }
    }
    
    // 폴백 모델로 시도
    try {
        return await fallbackCall();
    } catch (fallbackError) {
        throw new Error(`모든 모델 시도 실패: ${lastError.message}, ${fallbackError.message}`);
    }
}

function isRetryableError(error) {
    const retryableCodes = [
        'rate_limit_exceeded',
        'timeout',
        'network_error',
        'service_unavailable'
    ];
    
    return retryableCodes.includes(error.code);
}
```

## **성능 모니터링**

### **제공자별 성능 추적**
```javascript
// ✅ DO: 제공자별 성능 모니터링
class ProviderPerformanceMonitor {
    constructor() {
        this.metrics = new Map();
    }
    
    recordCall(provider, model, startTime, endTime, success, tokens) {
        const key = `${provider}:${model}`;
        
        if (!this.metrics.has(key)) {
            this.metrics.set(key, {
                totalCalls: 0,
                successfulCalls: 0,
                failedCalls: 0,
                totalTokens: 0,
                averageResponseTime: 0,
                totalCost: 0
            });
        }
        
        const metric = this.metrics.get(key);
        const responseTime = endTime - startTime;
        
        metric.totalCalls++;
        metric.totalTokens += tokens;
        metric.averageResponseTime = 
            (metric.averageResponseTime * (metric.totalCalls - 1) + responseTime) / metric.totalCalls;
        
        if (success) {
            metric.successfulCalls++;
            metric.totalCost += this.calculateCost(provider, model, tokens);
        } else {
            metric.failedCalls++;
        }
    }
    
    calculateCost(provider, model, tokens) {
        const costRates = {
            'anthropic:claude-3-sonnet-20240229': 0.003 / 1000,
            'openai:gpt-4o-mini': 0.00015 / 1000,
            'perplexity:llama-3.1-70b-instruct': 0.0002 / 1000
        };
        
        const rate = costRates[`${provider}:${model}`] || 0.001 / 1000;
        return tokens * rate;
    }
    
    getMetrics() {
        const result = {};
        
        for (const [key, metric] of this.metrics) {
            result[key] = {
                ...metric,
                successRate: metric.successfulCalls / metric.totalCalls,
                averageTokensPerCall: metric.totalTokens / metric.totalCalls
            };
        }
        
        return result;
    }
}
```

## **설정 관리**

### **모델 설정 업데이트**
```javascript
// ✅ DO: 동적 모델 설정 업데이트
class ModelConfigManager {
    constructor() {
        this.config = this.loadConfig();
    }
    
    loadConfig() {
        try {
            const configPath = path.join(process.cwd(), '.taskmaster', 'config.json');
            return JSON.parse(fs.readFileSync(configPath, 'utf8'));
        } catch (error) {
            return this.getDefaultConfig();
        }
    }
    
    getDefaultConfig() {
        return {
            models: {
                main: {
                    provider: 'anthropic',
                    model: 'claude-3-sonnet-20240229'
                },
                research: {
                    provider: 'perplexity',
                    model: 'llama-3.1-70b-instruct'
                },
                fallback: {
                    provider: 'openai',
                    model: 'gpt-4o-mini'
                }
            },
            parameters: {
                maxTokens: 2000,
                temperature: 0.7
            }
        };
    }
    
    updateModelConfig(role, provider, model) {
        this.config.models[role] = { provider, model };
        this.saveConfig();
    }
    
    saveConfig() {
        const configPath = path.join(process.cwd(), '.taskmaster', 'config.json');
        fs.writeFileSync(configPath, JSON.stringify(this.config, null, 2));
    }
}
```

## **테스트 및 검증**

### **제공자 테스트**
```javascript
// ✅ DO: AI 제공자 단위 테스트
describe('AI Providers', () => {
    let provider;
    
    beforeEach(() => {
        provider = new AnthropicProvider({
            apiKey: 'test-key'
        });
    });
    
    it('should generate text successfully', async () => {
        const result = await provider.generateText({
            prompt: 'Hello, world!',
            model: 'claude-3-sonnet-20240229',
            maxTokens: 100,
            temperature: 0.7
        });
        
        expect(result.text).toBeDefined();
        expect(result.usage).toBeDefined();
    });
    
    it('should handle API errors gracefully', async () => {
        // API 에러 시뮬레이션
        jest.spyOn(provider, 'callAnthropicAPI').mockRejectedValue({
            code: 'rate_limit_exceeded',
            message: 'Rate limit exceeded'
        });
        
        await expect(provider.generateText({
            prompt: 'Test',
            model: 'claude-3-sonnet-20240229'
        })).rejects.toThrow('속도 제한에 도달했습니다');
    });
});
```

### **모델 설정 검증**
```javascript
// ✅ DO: 모델 설정 유효성 검증
function validateModelConfig(config) {
    const errors = [];
    
    // 필수 역할 확인
    const requiredRoles = ['main', 'research', 'fallback'];
    for (const role of requiredRoles) {
        if (!config.models[role]) {
            errors.push(`필수 역할 '${role}'이 설정되지 않았습니다`);
        }
    }
    
    // 모델 유효성 확인
    for (const [role, modelConfig] of Object.entries(config.models)) {
        if (!modelConfig.provider || !modelConfig.model) {
            errors.push(`역할 '${role}'의 provider 또는 model이 설정되지 않았습니다`);
        }
    }
    
    // 파라미터 범위 확인
    if (config.parameters.temperature < 0 || config.parameters.temperature > 2) {
        errors.push('temperature는 0과 2 사이여야 합니다');
    }
    
    if (config.parameters.maxTokens < 1 || config.parameters.maxTokens > 100000) {
        errors.push('maxTokens는 1과 100000 사이여야 합니다');
    }
    
    return {
        isValid: errors.length === 0,
        errors
    };
}
```

---

**참고 파일:**
- [ai-providers/](mdc:src/ai-providers/) - AI 제공자 구현
- [config-manager.js](mdc:scripts/modules/config-manager.js) - 설정 관리
- [ai-services-unified.js](mdc:scripts/modules/ai-services-unified.js) - 통합 AI 서비스

8.  **Documentation:**
    -   Update any relevant documentation (like `README.md` or other rules) mentioning supported providers or configuration.

*(Note: For providers **without** an official Vercel AI SDK adapter, the process would involve directly using the provider's own SDK or API within the `src/ai-providers/<provider-name>.js` module and manually constructing responses compatible with the unified service layer, which is significantly more complex.)*